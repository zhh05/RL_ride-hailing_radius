{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhh\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import random\n",
    "import copy \n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "from drawnow import drawnow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from torch.distributions import MultivariateNormal,Beta,Normal\n",
    "\n",
    "from ride_hailing_env import RideHailingENV\n",
    "from ride_hailing_location_model import Build_Model\n",
    "\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import folium\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gym import spaces\n",
    "from folium.features import DivIcon\n",
    "from IPython.display import display\n",
    "from ride_hailing_match import Match\n",
    "from ride_hailing_location_model import Build_Model\n",
    "from pyproj import Transformer\n",
    "\n",
    "\n",
    "Global_Resolution = 7 # change the resolution here, determines the number of cells\n",
    "# !!!! change\n",
    "\n",
    "\n",
    "def wgs84_to_xy(x_arr: np.ndarray, y_arr: np.ndarray):\n",
    "    transformer = Transformer.from_crs('EPSG:4326', 'EPSG:32614')\n",
    "    x0 = 604082.94\n",
    "    y0 = 3328141.76\n",
    "    x_arr_new, y_arr_new = transformer.transform(x_arr, y_arr)\n",
    "    x_arr_new -= x0\n",
    "    y_arr_new -= y0\n",
    "    return x_arr_new.tolist(), y_arr_new.tolist()\n",
    "\n",
    "def xy_to_wgs84(xy_list):\n",
    "    transformer = Transformer.from_crs('EPSG:32614', 'EPSG:4326')\n",
    "    x_new = np.array(xy_list[0]) + 604082.94\n",
    "    y_new = np.array(xy_list[1]) + 3328141.76\n",
    "    lat_lon = transformer.transform(x_new, y_new)\n",
    "    return lat_lon\n",
    "\n",
    "def xy_to_wgs84_list(xy_list):\n",
    "    transformer = Transformer.from_crs('EPSG:32614', 'EPSG:4326')\n",
    "    x_arr = np.array(xy_list[0]) + 604082.94\n",
    "    y_arr = np.array(xy_list[1]) + 3328141.76\n",
    "    lat_list, lon_list = transformer.transform(x_arr, y_arr)\n",
    "    return lat_list.tolist(), lon_list.tolist()\n",
    "\n",
    "\n",
    "class Cell:\n",
    "    \"\"\"Gennerate cells\n",
    "    \"\"\"\n",
    "    def __init__(self, num_divisions) -> None:\n",
    "        self.lat_range = np.array([30.18, 30.32]) # Austin latitude range\n",
    "        self.lon_range = np.array([-97.81, -97.65]) # Austin longitude range\n",
    "        self.x_range, self.y_range = wgs84_to_xy(self.lat_range, self.lon_range)\n",
    "        self.num_divisions = num_divisions # how many part lat and lon are divided\n",
    "        pass\n",
    "   \n",
    "    def pass_info(self):\n",
    "        return self.lat_range, self.lon_range, self.x_range, self.y_range, self.num_divisions\n",
    "    \n",
    "    def get_cells(self, display_map: bool = False) -> list:\n",
    "        number =  self.num_divisions ** 2\n",
    "        cells = np.arange(number)\n",
    "\n",
    "        if display_map == False:\n",
    "            return cells\n",
    "        \n",
    "        else:\n",
    "            lat_step = (self.lat_range[1] - self.lat_range[0]) / self.num_divisions\n",
    "            lon_step = (self.lon_range[1] - self.lon_range[0]) / self.num_divisions\n",
    "            m = folium.Map(location=[(self.lat_range[0] + self.lat_range[1]) / 2, (self.lon_range[0] + self.lon_range[1]) / 2], zoom_start=13)\n",
    "            for i in range(self.num_divisions):\n",
    "                for j in range(self.num_divisions):\n",
    "                    lat_start = self.lat_range[0] + i * lat_step\n",
    "                    lat_end = lat_start + lat_step\n",
    "                    lon_start = self.lon_range[0] + j * lon_step\n",
    "                    lon_end = lon_start + lon_step\n",
    "                    grid_number = i * self.num_divisions + j\n",
    "\n",
    "                    # Draw the grid\n",
    "                    folium.Rectangle(\n",
    "                        bounds=[[lat_start, lon_start], [lat_end, lon_end]],\n",
    "                        color='blue',\n",
    "                        fill=True,\n",
    "                        fill_opacity=0.1\n",
    "                    ).add_to(m)\n",
    "\n",
    "                    # Add grid number\n",
    "                    folium.Marker(\n",
    "                        location=[(lat_start + lat_end) / 2, (lon_start + lon_end) / 2],\n",
    "                        icon=folium.DivIcon(html=f'<div style=\"font-size: 12pt\">{grid_number}</div>')\n",
    "                    ).add_to(m)\n",
    "\n",
    "            # draw action range\n",
    "            folium.Circle(\n",
    "                radius=50,\n",
    "                location=[(self.lat_range[1]-self.lat_range[0])/2+self.lat_range[0], (self.lon_range[1]-self.lon_range[0])/2+self.lon_range[0]],\n",
    "                color=\"red\",\n",
    "                weight=5,\n",
    "                fill=False,\n",
    "            ).add_to(m)\n",
    "            folium.Circle(\n",
    "                radius=3000,\n",
    "                location=[(self.lat_range[1]-self.lat_range[0])/2+self.lat_range[0], (self.lon_range[1]-self.lon_range[0])/2+self.lon_range[0]],\n",
    "                color=\"red\",\n",
    "                weight=5,\n",
    "                fill=False,\n",
    "            ).add_to(m)\n",
    "\n",
    "            display(m)\n",
    "            return cells\n",
    "\n",
    "    def get_cell_id_wgs84(self, lat, lon):\n",
    "        if not (self.lat_range[0] <= lat <= self.lat_range[1]) or not (self.lon_range[0] <= lon <= self.lon_range[1]): # check if in the range\n",
    "            return None\n",
    "        lat_step = (self.lat_range[1] - self.lat_range[0]) / self.num_divisions\n",
    "        lon_step = (self.lon_range[1] - self.lon_range[0]) / self.num_divisions\n",
    "        lat_index = min(int((lat - self.lat_range[0]) / lat_step), self.num_divisions - 1)\n",
    "        lon_index = min(int((lon - self.lon_range[0]) / lon_step), self.num_divisions - 1)\n",
    "        grid_number = lat_index * self.num_divisions + lon_index\n",
    "        return grid_number\n",
    "    \n",
    "    def get_cell_id_xy(self, x_list, y_list):\n",
    "        cell_ids = []\n",
    "        x_step = (self.x_range[1] - self.x_range[0]) / self.num_divisions\n",
    "        y_step = (self.y_range[1] - self.y_range[0]) / self.num_divisions\n",
    "\n",
    "        for x, y in zip(x_list, y_list):\n",
    "            x_index = min(int((x - self.x_range[0]) / x_step), self.num_divisions - 1)\n",
    "            y_index = min(int((y - self.y_range[0]) / y_step), self.num_divisions - 1)\n",
    "            grid_number = x_index + y_index * self.num_divisions\n",
    "            cell_ids.append(grid_number)\n",
    "        \n",
    "        return cell_ids\n",
    "        \n",
    "\n",
    "class Gen_Model:\n",
    "    \"\"\"Sample locations from fitted model for riders and drivers in the map\n",
    "\n",
    "    This class is to generate locations for riders and drivers based on the given\n",
    "    distribution of their locations. This default model is estimated with Kernel \n",
    "    Density Estimation (KDE). The generated location is given in the format of \n",
    "    a pandas DataFrame, each row represents a unique rider/driver. Information\n",
    "    given in a row includes rider/driver's ID, H3 code, longitude and latitude.\n",
    "\n",
    "    Attributes:\n",
    "        model: an instance of Build_Model class containing models for riders and drivers\n",
    "        rider_model: a dictionary for 24 KDE distributions, describing the locational and timely distribution of riders\n",
    "        driver_model: a dictionary for 24 KDE distributions, describing the locational and timely distribution of drivers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_div) -> None:\n",
    "        self.cell = Cell(num_div)\n",
    "        self.cell_ids = self.cell.get_cells(False)\n",
    "        self.model = Build_Model()\n",
    "        self.rider_model, self.driver_model = self.model.get_model()\n",
    "\n",
    "    def gen_drivers(self, number_of_drivers: int, hr_time: int, seed: int = None):\n",
    "        \"\"\"Sample locations for drivers\n",
    "        \n",
    "        Sample multiple locations for drivers based on the given locational distribution \n",
    "        of drivers. The default distribution model is KDE.\n",
    "\n",
    "        Parameters:\n",
    "            number_of_drivers: an int, indicating how many drivers are generated.\n",
    "            hr_time: an int, the value is the hour of the day, indicating which distribution model will be used.\n",
    "            resolution: an int, determines the number of cells in the map.\n",
    "\n",
    "        Returns:\n",
    "            driver_df: a pandas DataFrame, including information of generated drivers. \n",
    "        \"\"\"\n",
    "        if seed != None:\n",
    "            driver_locations = self.model.sample_from_model(self.driver_model[f'{hr_time}'], number_of_drivers, seed) # dtype = numpy ndarray\n",
    "        else:\n",
    "            driver_locations = self.model.sample_from_model(self.driver_model[f'{hr_time}'], number_of_drivers) # dtype = numpy ndarray\n",
    "        driver_ids = []\n",
    "        cell_ids = []\n",
    "\n",
    "        for driver_id, geo_info in enumerate(driver_locations):\n",
    "            cell_id = self.cell.get_cell_id_wgs84(geo_info[0], geo_info[1])\n",
    "            while cell_id is None:\n",
    "                geo_info = self.model.sample_from_model(self.rider_model[f'{hr_time}'], 1)[0] # dtype = numpy ndarray\n",
    "                cell_id = self.cell.get_cell_id_wgs84(geo_info[0], geo_info[1])\n",
    "                driver_locations[driver_id] = geo_info\n",
    "                #print('Re-sampled a driver!')\n",
    "            cell_ids.append(cell_id)\n",
    "            driver_ids.append(driver_id)\n",
    "\n",
    "        x_list, y_list = wgs84_to_xy(driver_locations.T[0], driver_locations.T[1])\n",
    "\n",
    "        driver_df = pd.DataFrame({'driver_id':driver_ids, 'cell_id':cell_ids, 'x':x_list, 'y':y_list, 'statue': 1, 'idle_time': 0})\n",
    "        # driver is avliable: 'statue' = 1, unavliable: 'statue' = 0\n",
    "        # driver_df[['driver_id', 'cell_id']] = driver_df[['driver_id', 'cell_id']].astype(int) # set data type to int\n",
    "\n",
    "        return driver_df\n",
    "\n",
    "    def gen_riders(self, number_of_riders: int, hr_time: int, seed: int = None):\n",
    "        \"\"\"Sample locations for riders\n",
    "        \n",
    "        Sample multiple locations for riders based on the given locational distribution \n",
    "        of riders. The default distribution model is KDE.\n",
    "\n",
    "        Parameters:\n",
    "            number_of_riders: an int, indicating how many riders are generated.\n",
    "            hr_time: an int, the value is the hour of the day, indicating which distribution model will be used.\n",
    "            resolution: an int, determines the number of cells in the map.\n",
    "\n",
    "        Returns:\n",
    "            rider_df: a pandas DataFrame, including information of generated riders. \n",
    "        \"\"\"\n",
    "        if seed != None:\n",
    "            rider_locations = self.model.sample_from_model(self.rider_model[f'{hr_time}'], number_of_riders, seed) # dtype = numpy ndarray\n",
    "        else:\n",
    "            rider_locations = self.model.sample_from_model(self.rider_model[f'{hr_time}'], number_of_riders) # dtype = numpy ndarray\n",
    "        rider_ids = []\n",
    "        cell_ids = []\n",
    "        \n",
    "        for rider_id, geo_info in enumerate(rider_locations):\n",
    "            cell_id = self.cell.get_cell_id_wgs84(geo_info[0], geo_info[1])\n",
    "            while cell_id is None:\n",
    "                geo_info = self.model.sample_from_model(self.rider_model[f'{hr_time}'], 1)[0] # dtype = numpy ndarray\n",
    "                cell_id = self.cell.get_cell_id_wgs84(geo_info[0], geo_info[1])\n",
    "                rider_locations[rider_id] = geo_info\n",
    "                #print('Re-sampled a rider!')\n",
    "            cell_ids.append(cell_id)\n",
    "            rider_ids.append(rider_id)\n",
    "\n",
    "        x_list, y_list = wgs84_to_xy(rider_locations.T[0], rider_locations.T[1])\n",
    "\n",
    "        rider_df = pd.DataFrame({'rider_id':rider_ids, 'cell_id':cell_ids, 'x':x_list, 'y':y_list, 'time_step_in_pool': 1})\n",
    "        # rider_df[['rider_id', 'cell_id', 'time_step_in_pool']] = rider_df[['rider_id', 'cell_id', 'time_step_in_pool']].astype(int) # set data type to int\n",
    "\n",
    "        return rider_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RideHailingENV(gym.Env):\n",
    "    \"\"\"Simulation environment for project optimising matching radius for ride-hailing system\n",
    "\n",
    "    This class is the main simulator for the master thesis project optimising matching radius \n",
    "    for a ride-hailing system with reinforcement learning. The project is carried out in TU Delft. \n",
    "    This simulator is built base on the geographical information of Austin, Texas, USA. It intake\n",
    "    continous matching radius as the action, and the reward is the total net profit made by the \n",
    "    system within a day.\n",
    "\n",
    "    GOOD action for example:\n",
    "    action = np.ones(36)*800\n",
    "    action[[5, 13, 14, 15, 19, 20, 21, 26, 27]] = 400\n",
    "\n",
    "    Attributes:\n",
    "     lower_bound - lower bound of action space, minimum matching radius, unit is meters.\n",
    "     upper_bound - upper bound of action space, maximum matching radius, unit is meters.\n",
    "     model - make an instance of Gen_Model class, to generate riders and drivers for the simulator.\n",
    "     match - make an instance of Match class, to run the matching algorithm.\n",
    "     radius_initial - initial matching radius when reset the environment, unit is meters.\n",
    "     driver_num_ini - initial number of drivers, can be changed if set dynamic.\n",
    "     rider_num_ini - initial number of riders, rider number is changing among different steps.\n",
    "     fuel_unit_price - average travelling fuel cost per vehicle per kilometer in the US, the unit is US dollars.\n",
    "     time_window - time interval between every two matching process (Uber Batched matching), fixed among all steps, unit is minutes.\n",
    "     total_reward - total reward for the intake action.\n",
    "     gen_rate_rider - overall generating rate of riders, number of riders per time-window.\n",
    "     gen_rate_driver - active if vehicle number are set dynamic, number of new drivers per time-window.\n",
    "     ride_price - average ride price in Austin urban area, the value is estimated from Uber ride data in 2022.\n",
    "     rider_patience - the maximum number of steps a rider can stay in the matching pool.\n",
    "     p_unmatch_rider - penalty per unmatched rider, the value is cauculated base on the probability of losing a potential ride.\n",
    "     action_space - defines the numerical range of intake actions.\n",
    "     observation_space - defines the numerical range of overall observations.\n",
    "     sub_observation_space - defines the numerical range of observations within a cell.\n",
    "\n",
    "    Distance cost explain:\n",
    "     distance cost = car-buying cost + car repair and maintanance cost + fuel cost\n",
    "     car repair and maintanance cost = change tire per 200000km &1000 + change motor oil per 200000km $1200\n",
    "     car-buying cost: average car price in the us is $22000\n",
    "     fuel cost: averge fuel cost $1.3/L * averge fuel comsuption 9.3L/100km = $12.1/100km = $0.12/km\n",
    "     total = $11.65/km\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_div, time_window: int = 0.25) -> None:\n",
    "\n",
    "        lower_bound = 50\n",
    "        upper_bound = 3000\n",
    "        self.num_divi = grid_div\n",
    "        self.cell = Cell(self.num_divi)\n",
    "        self.lat_range, self.lon_range, self.x_range, self.y_range, self.num_divisions = self.cell.pass_info()\n",
    "        self.model = Gen_Model(self.num_divi)\n",
    "        self.match = Match()\n",
    "        self.seed_ini = None\n",
    "\n",
    "        self.cell_ids = self.cell.get_cells(False)\n",
    "        self.cell_num = np.size(self.cell_ids)\n",
    "        self.radius_initial = 500\n",
    "        self.driver_num_ini = 100\n",
    "        self.rider_num_ini = 15\n",
    "        self.distance_cost = 13.26 * 0.001 # this is a combination of fuel cost, car-buying and car-repair cost, unit is per meter\n",
    "        self.fuel_cost = 1.93 * 0.001\n",
    "        self.time_window = time_window\n",
    "        self.total_reward = 0\n",
    "        self.gen_rate_rider = 10\n",
    "        self.gen_rate_driver = 10\n",
    "        self.ride_price = 23.92\n",
    "        self.rider_patience = 5 # minutes\n",
    "        self.p_rider_left = 0.1 # punishment\n",
    "        self.p_unmatch_rider = 5\n",
    "        self.simulation_time = 30\n",
    "\n",
    "        self.drivers = None\n",
    "        self.riders = None\n",
    "        self.drivers_tmp = None\n",
    "        self.riders_tmp = None\n",
    "\n",
    "        self.score_distance = lambda distance, max_distance=2000: max(0, min(1, 1 - distance / max_distance))\n",
    "        self.score_radius = lambda distance, max_distance=8000: max(0, min(1, 1 - distance / max_distance))\n",
    "        self.score_radius_neg = lambda distance, max_distance=8000: max(0, min(1, distance / max_distance))\n",
    "        self.score_diff = lambda distance, max_distance=1500: max(0, min(1, 1 - distance / max_distance))\n",
    "        self.score_rate = lambda rate: 1 if rate > 0.85 else (0 if rate < 0.5 else (rate - 0.5) / 0.4)\n",
    "        self.multi_task_weight_factor = [0.3, 0.6, 0.1] # order fulfillment rate, average pickup distance, driver utilization rate\n",
    "        #self.multi_task_weight = [0.6, 0.4] # order fulfillment rate, average pickup distance\n",
    "\n",
    "        self. rider_patience_step = self.rider_patience / self.time_window\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "                low=np.array(lower_bound*np.ones(self.cell_num)), \n",
    "                high=np.array(upper_bound*np.ones(self.cell_num)),\n",
    "                dtype=np.int32\n",
    "                )\n",
    "        self.observation_space = spaces.Discrete(self.num_divi ** 2) # updated\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.max_step = self.simulation_time / self.time_window\n",
    "\n",
    "    def reset(self, time_ini: int = 1) -> np.array:\n",
    "        \"\"\"\n",
    "        reset the environment for the first step in every episode.\n",
    "\n",
    "        Parameters:\n",
    "         time_ini -  set the initial time to 0-1 hour of a day.\n",
    "\n",
    "        Returns:\n",
    "         returns the initial matching radius.\n",
    "        \"\"\"\n",
    "        self.random_seed = self.seed_ini\n",
    "        self.drivers = self.model.gen_drivers(self.driver_num_ini, time_ini, 1)\n",
    "        self.drivers_in_service = np.zeros(self.driver_num_ini, dtype=np.int32)\n",
    "        self.drivers['driver_id'] = np.arange(self.drivers.shape[0])\n",
    "\n",
    "        self.riders = self.model.gen_riders(self.rider_num_ini, time_ini, self.random_seed)\n",
    "        self.riders['rider_id'] = np.arange(self.riders.shape[0])\n",
    "\n",
    "        done = False\n",
    "        self.step_count = 0\n",
    "\n",
    "        rider, driver, avg_dis = self.get_observe()\n",
    "        self.matched_ride_num = 0\n",
    "        self.ride_demand_num = self.rider_num_ini\n",
    "        self.avg_distance_pf = 0\n",
    "        self.driver_ult = 0\n",
    "        self.radius_base = avg_dis\n",
    "        self.action_ini = np.ones(self.cell_num) * 50\n",
    "        state = np.ones(self.cell_num)*500\n",
    "\n",
    "        return state, done #, self.riders, self.drivers\n",
    "    \n",
    "    def step(self, radius: np.array, hr_time: int, rend_step: bool = False) -> tuple[float, dict, list]:\n",
    "        \"\"\"\n",
    "        The main process of a step.\n",
    "\n",
    "        Parameters:\n",
    "         radius - matching radius for each cell.\n",
    "         hr_time - the hourly time step in a day, determines location distribution of riders and drivers.\n",
    "         rend_step - visualize one step if set to True.\n",
    "\n",
    "        Returns:\n",
    "         the reward of one step and matched pairs within this step. \n",
    "        \"\"\"\n",
    "\n",
    "        #print(self.x_range, self.y_range, self.drivers)\n",
    "        #self.random_seed += 1 # update seed\n",
    "        self.action_ini = radius\n",
    "        self.riders_tmp = self.riders.copy()\n",
    "        self.drivers_tmp = self.drivers.copy()\n",
    "\n",
    "        # get the distance matrix and matching pool\n",
    "        cell_ids = self.riders['cell_id']\n",
    "        r_radius = radius[cell_ids]\n",
    "\n",
    "        # get the matching pool\n",
    "        pool = self.__get_pool(self.dis_matrix, r_radius)\n",
    "\n",
    "        # matching process\n",
    "        match_statue = self.__match(pool)\n",
    "        reward = self.__execute_match(match_statue, pool, radius)\n",
    "        \n",
    "        if rend_step:\n",
    "            state = [self.riders_tmp, self.drivers_tmp]\n",
    "            self.render(state, radius, match_statue)\n",
    "\n",
    "        self.riders, self.drivers, reward, done = self.__state_transit(hr_time, reward, match_statue)\n",
    "\n",
    "        # give terminal reward\n",
    "        if done == True:\n",
    "            reward += 0 #100\n",
    "        else:\n",
    "            reward -= 0 #len(self.riders)\n",
    "            self.step_count += 1\n",
    "\n",
    "        rider, driver, avg_dis = self.get_observe()\n",
    "        return rider, driver, avg_dis, reward, done\n",
    "    \n",
    "    def get_observe(self): # observation is number of riders/drivers in each cell\n",
    "\n",
    "        # new version observation, according to ref: cell num, time_step, previoud radius, number of drivers, number of riders\n",
    "        rider_counts = self.riders['cell_id'].value_counts().sort_index().reindex(range(self.cell_num), fill_value=0).to_numpy()\n",
    "        supply_driver = self.drivers[self.drivers['statue']==1]\n",
    "        driver_counts = supply_driver['cell_id'].value_counts().sort_index().reindex(range(self.cell_num), fill_value=0).to_numpy()\n",
    "\n",
    "        avg_distance = np.full(self.cell_num, 0)  # Initialize with 6000 for cells with no riders\n",
    "\n",
    "        rider_vec = self.riders[['x', 'y']].values\n",
    "        driver_vec = self.drivers[['x', 'y']].values\n",
    "\n",
    "        # get the distance matrix and matching pool\n",
    "        self.dis_matrix = self.__vector_dis(rider_vec, driver_vec)\n",
    "\n",
    "        # Calculate the minimum distance from each rider to any driver\n",
    "        for cell_id in range(self.cell_num):\n",
    "            riders_in_cell = self.riders[self.riders['cell_id'] == cell_id].index\n",
    "            if len(riders_in_cell) > 0:\n",
    "                distances = self.dis_matrix[riders_in_cell].min(axis=1)\n",
    "                distance = distances.mean()\n",
    "                avg_distance[cell_id] = distance\n",
    "        \n",
    "        self.avg_distance = np.clip(avg_distance, 0, 2500)\n",
    "        self.diff = abs(self.action_ini - self.radius_base)\n",
    "\n",
    "        return rider_counts, driver_counts, self.avg_distance, self.diff\n",
    "    \n",
    "    def check_done(self):\n",
    "        if self.step_count >= self.max_step -1: #or self.riders.shape[0] == 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return done\n",
    "\n",
    "    def __get_pool(self, dis_matrix: np.matrix, radius_set: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        form a matching pool for all the riders and available drivers whithin the matching radius.\n",
    "\n",
    "        Parameters:\n",
    "            riders - locations, numbers of all the riders.\n",
    "            drivers - locations, numbers of all the drivers.\n",
    "            radius - matching radius for each cell, riders in the same cell have the same matching radius.\n",
    "\n",
    "        Returns:\n",
    "            returns a list consist all the possible matches and the distance between them.\n",
    "        \"\"\"\n",
    "        match_pool = []\n",
    "        for i in range(dis_matrix.shape[0]):\n",
    "            sub_pool = []\n",
    "            radius_rider = radius_set[i]\n",
    "            driver_1 = np.where(dis_matrix[i] <= radius_rider)[1]\n",
    "            driver_2 = self.drivers.index[self.drivers['statue'] == 1]\n",
    "            driver = list(np.intersect1d(driver_1, driver_2))\n",
    "            rider = list(np.ones(np.size(driver), dtype=int)*i)\n",
    "            dis = dis_matrix[i, driver].tolist()[0]\n",
    "            sub_pool.extend([rider])\n",
    "            sub_pool.extend([driver])\n",
    "            sub_pool.extend([dis])\n",
    "            sub_pool = list(map(list, zip(*sub_pool)))\n",
    "            match_pool.extend(sub_pool)\n",
    "        return match_pool\n",
    "            \n",
    "    def __match(self, pool: list): # MM: Maximum Matching, OM: Optimised Matching\n",
    "        \"\"\"\n",
    "        excute matching algorithm to find the optimal match for the given matching pool.\n",
    "\n",
    "        Parameters:\n",
    "         pool - matching pool with distance. \n",
    "\n",
    "        Returns:\n",
    "         match statue with matched pairs and their distance. \n",
    "        \"\"\"\n",
    "        matched_pairs = self.match.match(pool, method='Munkres')\n",
    "        return matched_pairs\n",
    "    \n",
    "    def __execute_match(self, match_statue:list, match_pool: list, radius: np.array) -> tuple[float, tuple]:\n",
    "        \"\"\"\n",
    "        apply the matched pairs to the map, update riders and drivers, observe reward and penalty.\n",
    "\n",
    "        Parameters:\n",
    "        riders - locations, numbers of all the riders.\n",
    "        drivers - locations, numbers of all the drivers.\n",
    "        match_statue - matched pair of riders and drivers with the distance between them.\n",
    "\n",
    "        Returns:\n",
    "        reward - the net monetary profit made from the ride-hailing system within a step.\n",
    "        pool_next - next state of the environment after taking the action.\n",
    "        \"\"\"\n",
    "        # calculate total distance and rewards\n",
    "        reward = 0 #- 200 * self.distance_cost\n",
    "        complete_rate = self.score_rate(len(match_statue) / len(self.riders))\n",
    "        self.matched_ride_num += len(match_statue)\n",
    "        self.driver_ult += len(match_statue) / len(self.drivers[self.drivers['statue']==1])\n",
    "\n",
    "        if match_statue:\n",
    "            aver_distance = sum(distance for _, _, distance in match_statue) / len(match_statue)\n",
    "            riders_to_drop, drivers_to_drop = zip(*[(rider, driver) for rider, driver, _ in match_statue])\n",
    "            riders_to_drop, drivers_to_drop = list(riders_to_drop), list(drivers_to_drop)\n",
    "            #reward_match_rate = len(match_statue) / len(self.riders)\n",
    "            #reward_driver_ult = len(match_statue) / len(match_pool)\n",
    "        else:\n",
    "            aver_distance = 0 # in case for tricky policies\n",
    "            riders_to_drop, drivers_to_drop = [], []\n",
    "            #reward_match_rate = 0\n",
    "            #reward_driver_ult = 0\n",
    "                 \n",
    "        #reward_distance = self.score_distance(aver_distance)\n",
    "        self.avg_distance_pf += aver_distance\n",
    "        #print(complete_rate, reward_distance, reward_driver_ult)\n",
    "        #reward_diff = abs(radius - self.avg_distance)\n",
    "        base_radius_diff = sum(self.diff)\n",
    "\n",
    "        #reward = complete_rate * self.multi_task_weight_factor[0] + reward_distance * self.multi_task_weight_factor[1] + reward_driver_ult * self.multi_task_weight_factor[2]\n",
    "        #reward = complete_rate * self.multi_task_weight_factor[0] + self.score_diff(sum(reward_diff)/self.cell_num) * self.multi_task_weight_factor[1]\n",
    "        #reward = self.score_diff(sum(reward_diff)/self.cell_num) * self.multi_task_weight_factor[1]\n",
    "        #reward = complete_rate * self.multi_task_weight_factor[0] - self.score_radius_neg(sum(radius)) * self.multi_task_weight_factor[1]\n",
    "        #reward = complete_rate * self.multi_task_weight_factor[0]\n",
    "        reward = complete_rate * self.multi_task_weight_factor[0] + self.score_diff(sum(base_radius_diff)) * self.multi_task_weight_factor[1]\n",
    "\n",
    "        self.riders = self.riders.drop(riders_to_drop)\n",
    "        self.drivers_in_service[drivers_to_drop] += 2\n",
    "        self.drivers.loc[drivers_to_drop, 'statue'] = 0\n",
    "        self.drivers.loc[drivers_to_drop, 'idle_time'] = 0\n",
    "\n",
    "        self.riders['time_step_in_pool'] += 1\n",
    "        self.riders = self.riders.drop(self.riders[self.riders['time_step_in_pool']>self.rider_patience_step].index) # inpatient riders quit the matching pool\n",
    "\n",
    "        # calculate net reward\n",
    "        net_reward = reward\n",
    "\n",
    "        return net_reward\n",
    "    \n",
    "    def __vector_dis(self, rider_vec, driver_vec):\n",
    "        m = np.shape(rider_vec)[0]\n",
    "        n = np.shape(driver_vec)[0]\n",
    "        M = np.dot(rider_vec, driver_vec.T)\n",
    "        H = np.tile(np.matrix(np.square(rider_vec).sum(axis=1)).T,(1,n))\n",
    "        K = np.tile(np.matrix(np.square(driver_vec).sum(axis=1)),(m,1))\n",
    "        return np.sqrt(-2 * M + H + K)\n",
    "    \n",
    "    def __state_transit(self, hr_time: int, reward: float, match_statue: list) -> dict: \n",
    "        \"\"\"\n",
    "        update the current state and give the state of the next step.\n",
    "        v1_update: do not gennerate new riders, terminal is all the riders are matched or left\n",
    "\n",
    "        Parameters:\n",
    "         state - the current state, locations of riders and drivers.\n",
    "         hr_time - hourly time of a day, this is used to generate new riders and drivers.\n",
    "\n",
    "        Returns:\n",
    "         returns the locations of riders and drivers for the next step.\n",
    "        \"\"\"\n",
    "        ride_num = np.size(match_statue)\n",
    "\n",
    "        # update riders\n",
    "        done = self.check_done()\n",
    "        if done == True:\n",
    "            self.riders = self.riders.reset_index(drop=True)\n",
    "            self.drivers = self.drivers.reset_index(drop=True)\n",
    "            # re-index drivers and riders\n",
    "            driver_index = self.drivers.shape[0]\n",
    "            self.drivers['driver_id'] = np.arange(driver_index)\n",
    "            rider_index = self.riders.shape[0]\n",
    "            self.riders['rider_id'] = np.arange(rider_index)\n",
    "\n",
    "            return self.riders, self.drivers, reward, done\n",
    "\n",
    "        rider_size = self.gen_rate_rider\n",
    "        self.ride_demand_num += rider_size\n",
    "        new_riders = self.model.gen_riders(rider_size, hr_time, self.random_seed)\n",
    "        rider_next = pd.concat((self.riders, new_riders), axis=0)\n",
    "\n",
    "        self.drivers.loc[self.drivers_in_service == 0, 'statue'] = 1 # drivers finished ride\n",
    "\n",
    "        # update drivers - driver reposition\n",
    "        self.drivers.loc[self.drivers['statue'] == 1, 'idle_time'] += 1 # update idle time\n",
    "        condition = (self.drivers['idle_time'] == 20) & (self.drivers['statue'] == 1)\n",
    "        self.drivers.loc[condition, 'x'] += np.random.choice([-800, 800], size=condition.sum())\n",
    "        self.drivers.loc[condition, 'y'] += np.random.choice([-800, 800], size=condition.sum())\n",
    "        self.drivers.loc[condition, 'idle_time'] = 0 # reset idle time\n",
    "\n",
    "        # update drivers - driver idling\n",
    "        self.drivers_in_service[self.drivers_in_service != 0] -= 1\n",
    "        self.drivers['x'] += np.random.uniform(-400, 400, size=self.drivers.shape[0])\n",
    "        self.drivers['y'] += np.random.uniform(-400, 400, size=self.drivers.shape[0])\n",
    "\n",
    "        # check latitude and longitude border\n",
    "        self.drivers['x'] = np.clip(self.drivers['x'], self.x_range[0], self.x_range[1]) # check latitude range\n",
    "        self.drivers['y'] = np.clip(self.drivers['y'], self.y_range[0], self.y_range[1]) # check longitude range\n",
    "\n",
    "        # update cell ids for drivers\n",
    "        self.drivers['cell_id'] = self.cell.get_cell_id_xy(self.drivers['x'], self.drivers['y']) \n",
    "\n",
    "        #rider_next = rider_next.reset_index(drop=True)\n",
    "        self.riders = rider_next.reset_index(drop=True)\n",
    "        self.drivers = self.drivers.reset_index(drop=True)\n",
    "\n",
    "        # re-index drivers and riders\n",
    "        driver_index = self.drivers.shape[0]\n",
    "        self.drivers['driver_id'] = np.arange(driver_index)\n",
    "        rider_index = self.riders.shape[0]\n",
    "        self.riders['rider_id'] = np.arange(rider_index)\n",
    "\n",
    "        return self.riders, self.drivers, reward, done\n",
    "    \n",
    "    def get_performance_statue(self):\n",
    "        match_rate = self.matched_ride_num / (self.ride_demand_num - self.gen_rate_rider)\n",
    "        avg_pick_distance = self.avg_distance_pf / self.step_count\n",
    "        driver_ult = self.driver_ult / self.step_count\n",
    "        return match_rate, avg_pick_distance, driver_ult\n",
    "\n",
    "    def render(self, state: tuple, radius_set: dict, match_statue: list, color_set: tuple = ['red', 'blue'], folium_map=None) -> None:\n",
    "        \"\"\"\n",
    "        visualise the state and action for one step, red circle is matching range (within matching radius),\n",
    "        green lines are the links for matched pairs.\n",
    "\n",
    "        Parameters:\n",
    "         state - the current state, locations of riders and drivers.\n",
    "         radius_set - matching radius for each cell.\n",
    "         match_statue - matched pair of riders and drivers with the distance between them.\n",
    "         folium_map - map object.\n",
    "        \"\"\"\n",
    "\n",
    "        riders = state[0]\n",
    "        drivers = state[1]\n",
    "        drivers = drivers[drivers['statue'] == 1]\n",
    "        drivers.reset_index(drop=True, inplace=True)\n",
    "        matched_riders = []\n",
    "        matched_drivers = []\n",
    "        if match_statue != []:\n",
    "            matched_riders = pd.DataFrame(match_statue)[0].to_list()\n",
    "            matched_drivers = pd.DataFrame(match_statue)[1].to_list()\n",
    "\n",
    "        matched_rider_location = {}\n",
    "        matched_driver_location = {}\n",
    "\n",
    "        lat_step = (self.lat_range[1] - self.lat_range[0]) / self.num_divisions\n",
    "        lon_step = (self.lon_range[1] - self.lon_range[0]) / self.num_divisions\n",
    "        m = folium.Map(location=[(self.lat_range[0] + self.lat_range[1]) / 2, (self.lon_range[0] + self.lon_range[1]) / 2], zoom_start=13)\n",
    "        for i in range(self.num_divisions):\n",
    "            for j in range(self.num_divisions):\n",
    "                lat_start = self.lat_range[0] + i * lat_step\n",
    "                lat_end = lat_start + lat_step\n",
    "                lon_start = self.lon_range[0] + j * lon_step\n",
    "                lon_end = lon_start + lon_step\n",
    "                grid_number = i * self.num_divisions + j\n",
    "                # Draw the grid\n",
    "                folium.Rectangle(\n",
    "                    bounds=[[lat_start, lon_start], [lat_end, lon_end]],\n",
    "                    color='blue',\n",
    "                    fill=True,\n",
    "                    fill_opacity=0.1\n",
    "                ).add_to(m)\n",
    "                # Add grid number\n",
    "                folium.Marker(\n",
    "                    location=[(lat_start + lat_end) / 2, (lon_start + lon_end) / 2],\n",
    "                    icon=folium.DivIcon(html=f'<div style=\"font-size: 18pt\">{grid_number}</div>')\n",
    "                ).add_to(m)\n",
    "       \n",
    "        # add driver markers\n",
    "        for i in range(drivers.shape[0]):\n",
    "            driver_wgs = xy_to_wgs84([drivers.loc[i]['x'], drivers.loc[i]['y']])\n",
    "            folium.Marker(\n",
    "                location=driver_wgs,\n",
    "                icon=folium.Icon(\n",
    "                    color=color_set[0],\n",
    "                    prefix='fa',\n",
    "                    icon='car'\n",
    "                    )\n",
    "                ).add_to(m)\n",
    "            if drivers.loc[i]['driver_id'] in matched_drivers:\n",
    "                matched_driver_location[f'{int(drivers.loc[i][\"driver_id\"])}'] = driver_wgs\n",
    "          \n",
    "        # add rider markers and matching radius\n",
    "        for j in range(riders.shape[0]):\n",
    "            rider_wgs = xy_to_wgs84([riders.loc[j]['x'], riders.loc[j]['y']])\n",
    "            folium.Marker(\n",
    "                location=rider_wgs,\n",
    "                icon=folium.Icon(\n",
    "                    color=color_set[1],\n",
    "                    prefix='fa',\n",
    "                    icon='male'\n",
    "                    )\n",
    "                ).add_to(m)\n",
    "            \n",
    "            folium.Circle(\n",
    "                    radius=float(radius_set[int(riders.loc[j]['cell_id'])]),\n",
    "                    location=rider_wgs,\n",
    "                    color=\"red\",\n",
    "                    weight=1,\n",
    "                    fill=True,\n",
    "                    fill_opacity=0.1\n",
    "                ).add_to(m)\n",
    "            if riders.loc[j]['rider_id'] in matched_riders:\n",
    "                matched_rider_location[f'{int(riders.loc[j][\"rider_id\"])}'] = rider_wgs\n",
    "       \n",
    "        for rider, driver, dis in match_statue:\n",
    "            folium.PolyLine(\n",
    "                locations=[matched_rider_location[f'{int(rider)}'], matched_driver_location[f'{int(driver)}']],\n",
    "                color='green', \n",
    "                weight=5,\n",
    "                tooltip='matched_links'\n",
    "                ).add_to(m)\n",
    "    \n",
    "        display(m)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "  def __init__(self, memory_size=10000):\n",
    "    self.memory = deque(maxlen=memory_size)\n",
    "    self.memory_size = memory_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)\n",
    "\n",
    "  def append(self, item):\n",
    "    self.memory.append(item)\n",
    "\n",
    "  def sample_batch(self, batch_size):\n",
    "    idx = np.random.permutation(len(self.memory))[:batch_size]\n",
    "    return [self.memory[i] for i in idx]\n",
    "\n",
    "# Simple Ornstein-Uhlenbeck Noise generator\n",
    "class OUNoise(object):\n",
    "  \"\"\" Ornstein-Uhlenbeck process noise \"\"\"\n",
    "  def __init__(self, size, mu=0.0, theta=0.1, sigma=0.1):\n",
    "        \"\"\" Initialize parameters and noise process \"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta \n",
    "        self.sigma = sigma\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "        \"\"\" Reset the interal state (= noise) to mean (mu). \"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "  def sample(self):\n",
    "        \"\"\" Update internal state and return it as a noise sample \"\"\"\n",
    "        self.state += self.theta * (self.mu - self.state) + self.sigma * np.random.standard_normal(self.size)\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Actor, self).__init__()\n",
    "    self.fc_1 = nn.Linear(state_dim, 64)\n",
    "    self.fc_2 = nn.Linear(64, 32)\n",
    "    self.fc_out = nn.Linear(32, action_dim, bias=False)\n",
    "    init.xavier_normal_(self.fc_1.weight)\n",
    "    init.xavier_normal_(self.fc_2.weight)\n",
    "    init.xavier_normal_(self.fc_out.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.elu(self.fc_1(x))\n",
    "    out = F.elu(self.fc_2(out))\n",
    "    out = F.tanh(self.fc_out(out))\n",
    "    return out\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    self.fc_state = nn.Linear(state_dim, 32)\n",
    "    self.fc_action = nn.Linear(action_dim, 32)\n",
    "    self.fc = nn.Linear(64, 128)\n",
    "    self.fc_value = nn.Linear(128, 1, bias=False)\n",
    "    init.xavier_normal_(self.fc_state.weight)\n",
    "    init.xavier_normal_(self.fc_action.weight)\n",
    "    init.xavier_normal_(self.fc.weight)\n",
    "    init.xavier_normal_(self.fc_value.weight)\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    out_s = F.elu(self.fc_state(state))\n",
    "    out_a = F.elu(self.fc_action(action))\n",
    "    out = torch.cat([out_s, out_a], dim=1)\n",
    "    out = F.elu(self.fc(out))\n",
    "    out = self.fc_value(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "  def __init__(self, args) -> None:\n",
    "    self.args = args\n",
    "    self.env = gym.make('MountainCarContinuous-v0')\n",
    "    self.action_dim = self.env.action_space.shape[0]\n",
    "    self.state_dim = self.env.observation_space.shape[0]\n",
    "\n",
    "    self.actor = Actor(self.state_dim, self.action_dim)\n",
    "    self.critic = Critic(self.state_dim, self.action_dim)\n",
    "    self.actor_target = Actor(self.state_dim, self.action_dim)\n",
    "    self.critic_target = Critic(self.state_dim, self.action_dim)\n",
    "\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict()) # initial target net weights from policy net\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict()) # initial target net weights from value net\n",
    "\n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.lr)\n",
    "\n",
    "    self.max_action = self.env.action_space.high[0]\n",
    "    self.min_action = self.env.action_space.low[0]\n",
    "\n",
    "    self.noise = OUNoise(size=1, theta=0.15, sigma=0.2) # import noise\n",
    "\n",
    "    self.last_score_plot = [0]\n",
    "    self.avg_score_plot = [0]\n",
    "\n",
    "    self.memory_main = Memory(memory_size=10000)\n",
    "    self.memory_good_act = Memory(memory_size=10000)\n",
    "\n",
    "    self.loss_check = []\n",
    "    self.rider_num = []\n",
    "    \n",
    "    pass\n",
    "\n",
    "  def get_action(self, actor_net, state):\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "      state = torch.from_numpy(state).float()\n",
    "    action = actor_net(state)\n",
    "    return action\n",
    "  \n",
    "  def get_radius(self, action): # re-scale action\n",
    "    radius = action * self.scale + self.axis\n",
    "    return radius\n",
    "\n",
    "  def get_q_value(self, critic_net, state, action):\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "      state = torch.from_numpy(state).float()\n",
    "    if not isinstance(action, torch.Tensor):\n",
    "      action = torch.from_numpy(action).float()\n",
    "    q_value = critic_net(state, action)\n",
    "    return q_value\n",
    "\n",
    "  def update_actor(self, state):\n",
    "    action = self.actor(state)\n",
    "    #action = torch.clamp(action, float(self.min_action), float(self.max_action))\n",
    "    q_value = -torch.mean(self.critic(state, action)) \n",
    "    self.actor_optimizer.zero_grad() # calculate the gradient to update actor\n",
    "    q_value.backward()\n",
    "    self.actor_optimizer.step()\n",
    "    pass\n",
    "\n",
    "  def update_critic(self, state, action, target):\n",
    "    q_value = self.critic(state, action)\n",
    "    loss = F.mse_loss(q_value, target) # minimize loss to update critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "    #check = loss.detach().numpy()\n",
    "    #self.loss_check.append(check)\n",
    "    pass\n",
    "\n",
    "  def soft_update(self, target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "      target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau) # weights target and source\n",
    "\n",
    "  def draw_fig(self):\n",
    "    plt.plot(self.last_score_plot, '-')\n",
    "    plt.plot(self.avg_score_plot, 'r-')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reinforcement Learning Process')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "  def run_ddpg(self):\n",
    "    state = self.env.reset()[0]\n",
    "\n",
    "    iteration_now = 0\n",
    "    iteration = 0\n",
    "    episode = 0\n",
    "    episode_score = 0\n",
    "    episode_steps = 0\n",
    "\n",
    "    memory_warmup = self.args.batch_size * 3\n",
    "\n",
    "    while episode < self.args.max_episode:\n",
    "      print('\\rIteration {} | Episode {} | Result -> '.format(iteration_now, episode), end='')\n",
    "      #action = np.array([1, 1, 1, 1, 1, 1, 1])\n",
    "      action = self.get_action(self.actor, state).detach().numpy()\n",
    "\n",
    "      # blend determinstic action with random action during exploration, noise will become samller during the process\n",
    "      if episode < self.args.max_explore_eps:\n",
    "        p = episode / self.args.max_explore_eps\n",
    "        action = action * p + (1 - p) * self.noise.sample()[0]\n",
    "\n",
    "      action = np.clip(action, self.min_action, self.max_action) # select valid action range\n",
    "\n",
    "      next_state, reward, done, _, _ = self.env.step(action)\n",
    "      reward = reward# re-sacle the reward for plot\n",
    "      self.memory_main.append([state, action, reward, next_state, done])\n",
    "      if reward > -0.001:\n",
    "        self.memory_good_act.append([state, action, reward, next_state, done]) # memory for good actions\n",
    "\n",
    "      if iteration >= memory_warmup:\n",
    "        memory_batch_0 = self.memory_main.sample_batch(int(self.args.batch_size * 0.7))\n",
    "        memory_batch_1 = self.memory_good_act.sample_batch(int(self.args.batch_size * 0.3))\n",
    "        memory_batch = memory_batch_0 + memory_batch_1\n",
    "\n",
    "        memory_batch = self.memory_main.sample_batch(int(self.args.batch_size))\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = map(lambda x: torch.tensor(x).float(), zip(*memory_batch))\n",
    "\n",
    "        action_next = self.get_action(self.actor_target, next_state_batch)\n",
    "\n",
    "        # using discounted reward as target q-value to update critic\n",
    "        Q_next = self.get_q_value(self.critic_target, next_state_batch, action_next).detach()\n",
    "        Q_target_batch = reward_batch[:, None] + self.args.gamma * (1 - done_batch[:, None]) * Q_next\n",
    "        self.update_critic(state_batch, action_batch, Q_target_batch)\n",
    "        self.update_actor(state_batch)\n",
    "\n",
    "        # soft update\n",
    "        self.soft_update(self.actor_target, self.actor, self.args.tau)\n",
    "        self.soft_update(self.critic_target, self.critic, self.args.tau)\n",
    "\n",
    "      episode_score += reward\n",
    "      episode_steps += 1\n",
    "      iteration_now += 1\n",
    "      iteration += 1\n",
    "\n",
    "      if done:\n",
    "        print('Episode {:03d} | Episode Score:{:.03f} | steps {}'.format(episode, episode_score, episode_steps))\n",
    "        #print(f'Policy now: {radius}')\n",
    "        self.avg_score_plot.append(self.avg_score_plot[-1] * 0.99 + episode_score * 0.01)\n",
    "        self.last_score_plot.append(episode_score)\n",
    "\n",
    "        episode += 1\n",
    "        episode_score = 0\n",
    "        episode_steps = 0\n",
    "        iteration_now = 0\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "        self.noise.reset()\n",
    "        self.rider_num=[]\n",
    "      else:\n",
    "        state = next_state # state tranist\n",
    "\n",
    "    #drawnow(self.draw_fig) # drawnow function is for dynamic update\n",
    "    self.draw_fig()\n",
    "    return state, self.actor\n",
    "  \n",
    "  def debug_info(self):\n",
    "    return self.loss_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 971 | Episode 0 | Result -> Episode 000 | Episode Score:87.614 | steps 972\n",
      "Iteration 629 | Episode 1 | Result -> Episode 001 | Episode Score:90.808 | steps 630\n",
      "Iteration 723 | Episode 2 | Result -> Episode 002 | Episode Score:89.317 | steps 724\n",
      "Iteration 469 | Episode 3 | Result -> Episode 003 | Episode Score:92.849 | steps 470\n",
      "Iteration 1901 | Episode 4 | Result -> Episode 004 | Episode Score:75.390 | steps 1902\n",
      "Iteration 2717 | Episode 5 | Result -> Episode 005 | Episode Score:64.759 | steps 2718\n",
      "Iteration 1288 | Episode 6 | Result -> Episode 006 | Episode Score:82.557 | steps 1289\n",
      "Iteration 1903 | Episode 7 | Result -> Episode 007 | Episode Score:70.873 | steps 1904\n",
      "Iteration 1236 | Episode 8 | Result -> Episode 008 | Episode Score:82.390 | steps 1237\n",
      "Iteration 727 | Episode 9 | Result -> Episode 009 | Episode Score:91.324 | steps 728\n",
      "Iteration 2158 | Episode 10 | Result -> Episode 010 | Episode Score:72.895 | steps 2159\n",
      "Iteration 911 | Episode 11 | Result -> Episode 011 | Episode Score:87.263 | steps 912\n",
      "Iteration 1252 | Episode 12 | Result -> Episode 012 | Episode Score:82.407 | steps 1253\n",
      "Iteration 603 | Episode 13 | Result -> Episode 013 | Episode Score:93.229 | steps 604\n",
      "Iteration 980 | Episode 14 | Result -> Episode 014 | Episode Score:90.456 | steps 981\n",
      "Iteration 485 | Episode 15 | Result -> Episode 015 | Episode Score:95.420 | steps 486\n",
      "Iteration 2986 | Episode 16 | Result -> Episode 016 | Episode Score:66.811 | steps 2987\n",
      "Iteration 1047 | Episode 17 | Result -> Episode 017 | Episode Score:88.709 | steps 1048\n",
      "Iteration 1065 | Episode 18 | Result -> Episode 018 | Episode Score:85.119 | steps 1066\n",
      "Iteration 658 | Episode 19 | Result -> Episode 019 | Episode Score:93.361 | steps 659\n",
      "Iteration 3716 | Episode 20 | Result -> Episode 020 | Episode Score:62.182 | steps 3717\n",
      "Iteration 1098 | Episode 21 | Result -> Episode 021 | Episode Score:83.076 | steps 1099\n",
      "Iteration 1116 | Episode 22 | Result -> Episode 022 | Episode Score:88.546 | steps 1117\n",
      "Iteration 598 | Episode 23 | Result -> Episode 023 | Episode Score:92.720 | steps 599\n",
      "Iteration 539 | Episode 24 | Result -> Episode 024 | Episode Score:90.705 | steps 540\n",
      "Iteration 410 | Episode 25 | Result -> Episode 025 | Episode Score:95.153 | steps 411\n",
      "Iteration 748 | Episode 26 | Result -> Episode 026 | Episode Score:87.788 | steps 749\n",
      "Iteration 806 | Episode 27 | Result -> Episode 027 | Episode Score:90.697 | steps 807\n",
      "Iteration 660 | Episode 28 | Result -> Episode 028 | Episode Score:91.878 | steps 661\n",
      "Iteration 693 | Episode 29 | Result -> Episode 029 | Episode Score:92.614 | steps 694\n",
      "Iteration 475 | Episode 30 | Result -> Episode 030 | Episode Score:94.169 | steps 476\n",
      "Iteration 588 | Episode 31 | Result -> Episode 031 | Episode Score:93.425 | steps 589\n",
      "Iteration 480 | Episode 32 | Result -> Episode 032 | Episode Score:95.002 | steps 481\n",
      "Iteration 626 | Episode 33 | Result -> Episode 033 | Episode Score:91.317 | steps 627\n",
      "Iteration 532 | Episode 34 | Result -> Episode 034 | Episode Score:92.751 | steps 533\n",
      "Iteration 441 | Episode 35 | Result -> Episode 035 | Episode Score:93.842 | steps 442\n",
      "Iteration 484 | Episode 36 | Result -> Episode 036 | Episode Score:94.178 | steps 485\n",
      "Iteration 511 | Episode 37 | Result -> Episode 037 | Episode Score:93.527 | steps 512\n",
      "Iteration 489 | Episode 38 | Result -> Episode 038 | Episode Score:93.156 | steps 490\n",
      "Iteration 388 | Episode 39 | Result -> Episode 039 | Episode Score:95.949 | steps 389\n",
      "Iteration 259 | Episode 40 | Result -> Episode 040 | Episode Score:96.418 | steps 260\n",
      "Iteration 577 | Episode 41 | Result -> Episode 041 | Episode Score:94.322 | steps 578\n",
      "Iteration 208 | Episode 42 | Result -> Episode 042 | Episode Score:95.957 | steps 209\n",
      "Iteration 395 | Episode 43 | Result -> Episode 043 | Episode Score:95.723 | steps 396\n",
      "Iteration 217 | Episode 44 | Result -> Episode 044 | Episode Score:95.665 | steps 218\n",
      "Iteration 372 | Episode 45 | Result -> Episode 045 | Episode Score:95.963 | steps 373\n",
      "Iteration 370 | Episode 46 | Result -> Episode 046 | Episode Score:94.700 | steps 371\n",
      "Iteration 416 | Episode 47 | Result -> Episode 047 | Episode Score:93.813 | steps 417\n",
      "Iteration 303 | Episode 48 | Result -> Episode 048 | Episode Score:95.798 | steps 304\n",
      "Iteration 309 | Episode 49 | Result -> Episode 049 | Episode Score:94.700 | steps 310\n",
      "Iteration 363 | Episode 50 | Result -> Episode 050 | Episode Score:95.110 | steps 364\n",
      "Iteration 347 | Episode 51 | Result -> Episode 051 | Episode Score:95.212 | steps 348\n",
      "Iteration 314 | Episode 52 | Result -> Episode 052 | Episode Score:95.700 | steps 315\n",
      "Iteration 230 | Episode 53 | Result -> Episode 053 | Episode Score:96.636 | steps 231\n",
      "Iteration 153 | Episode 54 | Result -> Episode 054 | Episode Score:95.851 | steps 154\n",
      "Iteration 157 | Episode 55 | Result -> Episode 055 | Episode Score:97.123 | steps 158\n",
      "Iteration 281 | Episode 56 | Result -> Episode 056 | Episode Score:95.476 | steps 282\n",
      "Iteration 226 | Episode 57 | Result -> Episode 057 | Episode Score:95.181 | steps 227\n",
      "Iteration 326 | Episode 58 | Result -> Episode 058 | Episode Score:95.876 | steps 327\n",
      "Iteration 265 | Episode 59 | Result -> Episode 059 | Episode Score:96.054 | steps 266\n",
      "Iteration 308 | Episode 60 | Result -> Episode 060 | Episode Score:94.501 | steps 309\n",
      "Iteration 223 | Episode 61 | Result -> Episode 061 | Episode Score:96.425 | steps 224\n",
      "Iteration 223 | Episode 62 | Result -> Episode 062 | Episode Score:96.187 | steps 224\n",
      "Iteration 186 | Episode 63 | Result -> Episode 063 | Episode Score:96.988 | steps 187\n",
      "Iteration 572 | Episode 64 | Result -> Episode 064 | Episode Score:94.073 | steps 573\n",
      "Iteration 298 | Episode 65 | Result -> Episode 065 | Episode Score:96.221 | steps 299\n",
      "Iteration 224 | Episode 66 | Result -> Episode 066 | Episode Score:95.886 | steps 225\n",
      "Iteration 367 | Episode 67 | Result -> Episode 067 | Episode Score:95.567 | steps 368\n",
      "Iteration 325 | Episode 68 | Result -> Episode 068 | Episode Score:95.401 | steps 326\n",
      "Iteration 385 | Episode 69 | Result -> Episode 069 | Episode Score:96.004 | steps 386\n",
      "Iteration 281 | Episode 70 | Result -> Episode 070 | Episode Score:96.357 | steps 282\n",
      "Iteration 308 | Episode 71 | Result -> Episode 071 | Episode Score:95.969 | steps 309\n",
      "Iteration 356 | Episode 72 | Result -> Episode 072 | Episode Score:95.128 | steps 357\n",
      "Iteration 292 | Episode 73 | Result -> Episode 073 | Episode Score:94.595 | steps 293\n",
      "Iteration 306 | Episode 74 | Result -> Episode 074 | Episode Score:97.128 | steps 307\n",
      "Iteration 337 | Episode 75 | Result -> Episode 075 | Episode Score:95.227 | steps 338\n",
      "Iteration 424 | Episode 76 | Result -> Episode 076 | Episode Score:93.966 | steps 425\n",
      "Iteration 172 | Episode 77 | Result -> Episode 077 | Episode Score:96.805 | steps 173\n",
      "Iteration 236 | Episode 78 | Result -> Episode 078 | Episode Score:96.496 | steps 237\n",
      "Iteration 229 | Episode 79 | Result -> Episode 079 | Episode Score:96.434 | steps 230\n",
      "Iteration 230 | Episode 80 | Result -> Episode 080 | Episode Score:94.626 | steps 231\n",
      "Iteration 193 | Episode 81 | Result -> Episode 081 | Episode Score:94.459 | steps 194\n",
      "Iteration 289 | Episode 82 | Result -> Episode 082 | Episode Score:95.443 | steps 290\n",
      "Iteration 228 | Episode 83 | Result -> Episode 083 | Episode Score:96.037 | steps 229\n",
      "Iteration 168 | Episode 84 | Result -> Episode 084 | Episode Score:96.698 | steps 169\n",
      "Iteration 260 | Episode 85 | Result -> Episode 085 | Episode Score:97.133 | steps 261\n",
      "Iteration 255 | Episode 86 | Result -> Episode 086 | Episode Score:96.559 | steps 256\n",
      "Iteration 233 | Episode 87 | Result -> Episode 087 | Episode Score:95.952 | steps 234\n",
      "Iteration 154 | Episode 88 | Result -> Episode 088 | Episode Score:95.751 | steps 155\n",
      "Iteration 285 | Episode 89 | Result -> Episode 089 | Episode Score:95.921 | steps 286\n",
      "Iteration 157 | Episode 90 | Result -> Episode 090 | Episode Score:95.862 | steps 158\n",
      "Iteration 158 | Episode 91 | Result -> Episode 091 | Episode Score:96.645 | steps 159\n",
      "Iteration 194 | Episode 92 | Result -> Episode 092 | Episode Score:95.576 | steps 195\n",
      "Iteration 162 | Episode 93 | Result -> Episode 093 | Episode Score:96.801 | steps 163\n",
      "Iteration 262 | Episode 94 | Result -> Episode 094 | Episode Score:96.229 | steps 263\n",
      "Iteration 254 | Episode 95 | Result -> Episode 095 | Episode Score:95.029 | steps 255\n",
      "Iteration 189 | Episode 96 | Result -> Episode 096 | Episode Score:96.263 | steps 190\n",
      "Iteration 150 | Episode 97 | Result -> Episode 097 | Episode Score:96.096 | steps 151\n",
      "Iteration 165 | Episode 98 | Result -> Episode 098 | Episode Score:96.168 | steps 166\n",
      "Iteration 164 | Episode 99 | Result -> Episode 099 | Episode Score:95.940 | steps 165\n",
      "Iteration 158 | Episode 100 | Result -> Episode 100 | Episode Score:96.057 | steps 159\n",
      "Iteration 161 | Episode 101 | Result -> Episode 101 | Episode Score:94.979 | steps 162\n",
      "Iteration 154 | Episode 102 | Result -> Episode 102 | Episode Score:94.244 | steps 155\n",
      "Iteration 162 | Episode 103 | Result -> Episode 103 | Episode Score:95.774 | steps 163\n",
      "Iteration 196 | Episode 104 | Result -> Episode 104 | Episode Score:95.550 | steps 197\n",
      "Iteration 152 | Episode 105 | Result -> Episode 105 | Episode Score:95.060 | steps 153\n",
      "Iteration 192 | Episode 106 | Result -> Episode 106 | Episode Score:96.447 | steps 193\n",
      "Iteration 191 | Episode 107 | Result -> Episode 107 | Episode Score:95.468 | steps 192\n",
      "Iteration 205 | Episode 108 | Result -> Episode 108 | Episode Score:93.176 | steps 206\n",
      "Iteration 216 | Episode 109 | Result -> Episode 109 | Episode Score:94.462 | steps 217\n",
      "Iteration 193 | Episode 110 | Result -> Episode 110 | Episode Score:95.104 | steps 194\n",
      "Iteration 149 | Episode 111 | Result -> Episode 111 | Episode Score:95.033 | steps 150\n",
      "Iteration 151 | Episode 112 | Result -> Episode 112 | Episode Score:95.672 | steps 152\n",
      "Iteration 152 | Episode 113 | Result -> Episode 113 | Episode Score:93.658 | steps 153\n",
      "Iteration 133 | Episode 114 | Result -> Episode 114 | Episode Score:96.117 | steps 134\n",
      "Iteration 174 | Episode 115 | Result -> Episode 115 | Episode Score:92.693 | steps 175\n",
      "Iteration 108 | Episode 116 | Result -> Episode 116 | Episode Score:95.324 | steps 109\n",
      "Iteration 157 | Episode 117 | Result -> Episode 117 | Episode Score:95.188 | steps 158\n",
      "Iteration 161 | Episode 118 | Result -> Episode 118 | Episode Score:92.965 | steps 162\n",
      "Iteration 149 | Episode 119 | Result -> Episode 119 | Episode Score:94.917 | steps 150\n",
      "Iteration 177 | Episode 120 | Result -> Episode 120 | Episode Score:94.658 | steps 178\n",
      "Iteration 94 | Episode 121 | Result -> Episode 121 | Episode Score:95.754 | steps 95\n",
      "Iteration 141 | Episode 122 | Result -> Episode 122 | Episode Score:95.184 | steps 142\n",
      "Iteration 149 | Episode 123 | Result -> Episode 123 | Episode Score:93.577 | steps 150\n",
      "Iteration 189 | Episode 124 | Result -> Episode 124 | Episode Score:94.156 | steps 190\n",
      "Iteration 152 | Episode 125 | Result -> Episode 125 | Episode Score:93.069 | steps 153\n",
      "Iteration 80 | Episode 126 | Result -> Episode 126 | Episode Score:96.008 | steps 81\n",
      "Iteration 127 | Episode 127 | Result -> Episode 127 | Episode Score:94.343 | steps 128\n",
      "Iteration 81 | Episode 128 | Result -> Episode 128 | Episode Score:96.003 | steps 82\n",
      "Iteration 131 | Episode 129 | Result -> Episode 129 | Episode Score:93.730 | steps 132\n",
      "Iteration 86 | Episode 130 | Result -> Episode 130 | Episode Score:95.209 | steps 87\n",
      "Iteration 90 | Episode 131 | Result -> Episode 131 | Episode Score:95.099 | steps 91\n",
      "Iteration 153 | Episode 132 | Result -> Episode 132 | Episode Score:91.600 | steps 154\n",
      "Iteration 76 | Episode 133 | Result -> Episode 133 | Episode Score:95.784 | steps 77\n",
      "Iteration 166 | Episode 134 | Result -> Episode 134 | Episode Score:90.230 | steps 167\n",
      "Iteration 116 | Episode 135 | Result -> Episode 135 | Episode Score:92.819 | steps 117\n",
      "Iteration 80 | Episode 136 | Result -> Episode 136 | Episode Score:95.345 | steps 81\n",
      "Iteration 88 | Episode 137 | Result -> Episode 137 | Episode Score:94.881 | steps 89\n",
      "Iteration 109 | Episode 138 | Result -> Episode 138 | Episode Score:93.057 | steps 110\n",
      "Iteration 97 | Episode 139 | Result -> Episode 139 | Episode Score:94.037 | steps 98\n",
      "Iteration 73 | Episode 140 | Result -> Episode 140 | Episode Score:95.486 | steps 74\n",
      "Iteration 123 | Episode 141 | Result -> Episode 141 | Episode Score:92.152 | steps 124\n",
      "Iteration 74 | Episode 142 | Result -> Episode 142 | Episode Score:95.301 | steps 75\n",
      "Iteration 112 | Episode 143 | Result -> Episode 143 | Episode Score:91.751 | steps 113\n",
      "Iteration 70 | Episode 144 | Result -> Episode 144 | Episode Score:95.328 | steps 71\n",
      "Iteration 112 | Episode 145 | Result -> Episode 145 | Episode Score:91.752 | steps 113\n",
      "Iteration 72 | Episode 146 | Result -> Episode 146 | Episode Score:95.210 | steps 73\n",
      "Iteration 117 | Episode 147 | Result -> Episode 147 | Episode Score:91.736 | steps 118\n",
      "Iteration 71 | Episode 148 | Result -> Episode 148 | Episode Score:95.130 | steps 72\n",
      "Iteration 111 | Episode 149 | Result -> Episode 149 | Episode Score:91.300 | steps 112\n",
      "Iteration 75 | Episode 150 | Result -> Episode 150 | Episode Score:94.948 | steps 76\n",
      "Iteration 111 | Episode 151 | Result -> Episode 151 | Episode Score:90.919 | steps 112\n",
      "Iteration 72 | Episode 152 | Result -> Episode 152 | Episode Score:95.015 | steps 73\n",
      "Iteration 110 | Episode 153 | Result -> Episode 153 | Episode Score:91.155 | steps 111\n",
      "Iteration 110 | Episode 154 | Result -> Episode 154 | Episode Score:90.955 | steps 111\n",
      "Iteration 110 | Episode 155 | Result -> Episode 155 | Episode Score:91.060 | steps 111\n",
      "Iteration 111 | Episode 156 | Result -> Episode 156 | Episode Score:91.141 | steps 112\n",
      "Iteration 113 | Episode 157 | Result -> Episode 157 | Episode Score:91.099 | steps 114\n",
      "Iteration 110 | Episode 158 | Result -> Episode 158 | Episode Score:91.040 | steps 111\n",
      "Iteration 113 | Episode 159 | Result -> Episode 159 | Episode Score:91.063 | steps 114\n",
      "Iteration 110 | Episode 160 | Result -> Episode 160 | Episode Score:91.180 | steps 111\n",
      "Iteration 110 | Episode 161 | Result -> Episode 161 | Episode Score:90.897 | steps 111\n",
      "Iteration 109 | Episode 162 | Result -> Episode 162 | Episode Score:91.093 | steps 110\n",
      "Iteration 109 | Episode 163 | Result -> Episode 163 | Episode Score:91.180 | steps 110\n",
      "Iteration 76 | Episode 164 | Result -> Episode 164 | Episode Score:94.766 | steps 77\n",
      "Iteration 114 | Episode 165 | Result -> Episode 165 | Episode Score:91.138 | steps 115\n",
      "Iteration 109 | Episode 166 | Result -> Episode 166 | Episode Score:91.212 | steps 110\n",
      "Iteration 109 | Episode 167 | Result -> Episode 167 | Episode Score:90.857 | steps 110\n",
      "Iteration 108 | Episode 168 | Result -> Episode 168 | Episode Score:91.079 | steps 109\n",
      "Iteration 109 | Episode 169 | Result -> Episode 169 | Episode Score:91.004 | steps 110\n",
      "Iteration 110 | Episode 170 | Result -> Episode 170 | Episode Score:90.773 | steps 111\n",
      "Iteration 111 | Episode 171 | Result -> Episode 171 | Episode Score:91.118 | steps 112\n",
      "Iteration 109 | Episode 172 | Result -> Episode 172 | Episode Score:90.806 | steps 110\n",
      "Iteration 111 | Episode 173 | Result -> Episode 173 | Episode Score:91.144 | steps 112\n",
      "Iteration 108 | Episode 174 | Result -> Episode 174 | Episode Score:91.117 | steps 109\n",
      "Iteration 110 | Episode 175 | Result -> Episode 175 | Episode Score:91.221 | steps 111\n",
      "Iteration 108 | Episode 176 | Result -> Episode 176 | Episode Score:91.076 | steps 109\n",
      "Iteration 108 | Episode 177 | Result -> Episode 177 | Episode Score:91.233 | steps 109\n",
      "Iteration 108 | Episode 178 | Result -> Episode 178 | Episode Score:91.190 | steps 109\n",
      "Iteration 110 | Episode 179 | Result -> Episode 179 | Episode Score:91.230 | steps 111\n",
      "Iteration 108 | Episode 180 | Result -> Episode 180 | Episode Score:91.228 | steps 109\n",
      "Iteration 109 | Episode 181 | Result -> Episode 181 | Episode Score:91.228 | steps 110\n",
      "Iteration 108 | Episode 182 | Result -> Episode 182 | Episode Score:91.119 | steps 109\n",
      "Iteration 111 | Episode 183 | Result -> Episode 183 | Episode Score:91.245 | steps 112\n",
      "Iteration 108 | Episode 184 | Result -> Episode 184 | Episode Score:90.925 | steps 109\n",
      "Iteration 109 | Episode 185 | Result -> Episode 185 | Episode Score:90.937 | steps 110\n",
      "Iteration 108 | Episode 186 | Result -> Episode 186 | Episode Score:90.980 | steps 109\n",
      "Iteration 108 | Episode 187 | Result -> Episode 187 | Episode Score:91.105 | steps 109\n",
      "Iteration 109 | Episode 188 | Result -> Episode 188 | Episode Score:90.784 | steps 110\n",
      "Iteration 110 | Episode 189 | Result -> Episode 189 | Episode Score:91.104 | steps 111\n",
      "Iteration 109 | Episode 190 | Result -> Episode 190 | Episode Score:90.817 | steps 110\n",
      "Iteration 110 | Episode 191 | Result -> Episode 191 | Episode Score:91.259 | steps 111\n",
      "Iteration 110 | Episode 192 | Result -> Episode 192 | Episode Score:91.165 | steps 111\n",
      "Iteration 71 | Episode 193 | Result -> Episode 193 | Episode Score:95.087 | steps 72\n",
      "Iteration 75 | Episode 194 | Result -> Episode 194 | Episode Score:94.984 | steps 76\n",
      "Iteration 78 | Episode 195 | Result -> Episode 195 | Episode Score:94.909 | steps 79\n",
      "Iteration 110 | Episode 196 | Result -> Episode 196 | Episode Score:91.169 | steps 111\n",
      "Iteration 108 | Episode 197 | Result -> Episode 197 | Episode Score:90.991 | steps 109\n",
      "Iteration 83 | Episode 198 | Result -> Episode 198 | Episode Score:94.761 | steps 84\n",
      "Iteration 109 | Episode 199 | Result -> Episode 199 | Episode Score:91.267 | steps 110\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.43750325,  0.        ], dtype=float32),\n",
       " Actor(\n",
       "   (fc_1): Linear(in_features=2, out_features=64, bias=True)\n",
       "   (fc_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "   (fc_out): Linear(in_features=32, out_features=1, bias=False)\n",
       " ))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch DDPG solution of Ride_hailing Radius')\n",
    "\n",
    "parser.add_argument('--gamma', type=float, default=0.99)\n",
    "parser.add_argument('--lr', type=float, default=0.0001) #!\n",
    "parser.add_argument('--tau', type=float, default=0.001) # critic output weights between critic and target networks\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--max_episode', type=int, default=200)\n",
    "parser.add_argument('--max_explore_eps', type=int, default=150)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "ddpg = DDPG(args)\n",
    "ddpg.run_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 4, 7])\n",
    "b = np.array([3, 3, 3])\n",
    "\n",
    "a[a > b] = b[a > b]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.classic_control import utils\n",
    "\n",
    "\n",
    "class SimpleEnv(gym.Env):\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        self.min_action = np.ones(9)*(-10)\n",
    "        self.max_action = np.ones(9)*(10)\n",
    "        self.min_position = -100\n",
    "        self.max_position = 1500\n",
    "        self.min_speed = -10\n",
    "        self.max_speed = 10\n",
    "        self.goal_position = [500, 100, 120, 1000, 700, 600, 300, 400, 200]\n",
    "        self.power = 0.0015\n",
    "\n",
    "        self.low_state = np.ones(9)*self.min_position\n",
    "        self.high_state = np.ones(9)*self.max_position\n",
    "\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=self.min_action, high=self.max_action, shape=(9,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=self.low_state, high=self.high_state, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "\n",
    "        position = self.state\n",
    "\n",
    "        position += action * 100\n",
    "\n",
    "        position[position > self.high_state] = self.high_state[position > self.high_state]\n",
    "        position[position < self.low_state] = self.low_state[position < self.low_state]\n",
    "\n",
    "        # Convert a possible numpy bool to a Python bool.\n",
    "        terminated = bool(\n",
    "            (position >= self.goal_position).all()\n",
    "        )\n",
    "        reward = 0\n",
    "        if terminated:\n",
    "            reward = 100.0\n",
    "        reward -= math.pow(sum(action), 2) * 0.001\n",
    "        self.state = position\n",
    "\n",
    "        return self.state, reward, terminated\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        self.state = np.random.uniform(low=self.min_position, high=self.max_position, size=9)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "  def __init__(self, memory_size=10000):\n",
    "    self.memory = deque(maxlen=memory_size)\n",
    "    self.memory_size = memory_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)\n",
    "\n",
    "  def append(self, item):\n",
    "    self.memory.append(item)\n",
    "\n",
    "  def sample_batch(self, batch_size):\n",
    "    idx = np.random.permutation(len(self.memory))[:batch_size]\n",
    "    return [self.memory[i] for i in idx]\n",
    "\n",
    "# Simple Ornstein-Uhlenbeck Noise generator\n",
    "class OUNoise(object):\n",
    "  \"\"\" Ornstein-Uhlenbeck process noise \"\"\"\n",
    "  def __init__(self, size, mu=0.0, theta=0.1, sigma=0.1):\n",
    "        \"\"\" Initialize parameters and noise process \"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta \n",
    "        self.sigma = sigma\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "        \"\"\" Reset the interal state (= noise) to mean (mu). \"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "  def sample(self):\n",
    "        \"\"\" Update internal state and return it as a noise sample \"\"\"\n",
    "        self.state += self.theta * (self.mu - self.state) + self.sigma * np.random.standard_normal(self.size)\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, state_dim=1, action_dim=20):\n",
    "    super(Actor, self).__init__()\n",
    "    self.fc_1 = nn.Linear(state_dim, 32)\n",
    "    self.fc_2 = nn.Linear(32, 16)\n",
    "    self.fc_out = nn.Linear(16, action_dim, bias=False)\n",
    "    init.xavier_normal_(self.fc_1.weight)\n",
    "    init.xavier_normal_(self.fc_2.weight)\n",
    "    init.xavier_normal_(self.fc_out.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = F.elu(self.fc_1(x))\n",
    "    out = F.elu(self.fc_2(out))\n",
    "    out = F.tanh(self.fc_out(out))\n",
    "    return out\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, state_dim=1, action_dim=20):\n",
    "    super(Critic, self).__init__()\n",
    "    self.fc_state = nn.Linear(state_dim, 16)\n",
    "    self.fc_action = nn.Linear(action_dim, 16)\n",
    "    self.fc = nn.Linear(32, 32)\n",
    "    self.fc_value = nn.Linear(32, 1, bias=False)\n",
    "    init.xavier_normal_(self.fc_state.weight)\n",
    "    init.xavier_normal_(self.fc_action.weight)\n",
    "    init.xavier_normal_(self.fc.weight)\n",
    "    init.xavier_normal_(self.fc_value.weight)\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    out_s = F.elu(self.fc_state(state))\n",
    "    out_a = F.elu(self.fc_action(action))\n",
    "    out = torch.cat([out_s, out_a], dim=1)\n",
    "    out = F.elu(self.fc(out))\n",
    "    out = self.fc_value(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "  def __init__(self, args) -> None:\n",
    "    self.args = args\n",
    "    self.env = SimpleEnv()\n",
    "    self.action_dim = self.env.action_space.shape[0]\n",
    "    self.state_dim = self.env.observation_space.shape[0]\n",
    "\n",
    "    self.actor = Actor(self.state_dim, self.action_dim)\n",
    "    self.critic = Critic(self.state_dim, self.action_dim)\n",
    "    self.actor_target = Actor(self.state_dim, self.action_dim)\n",
    "    self.critic_target = Critic(self.state_dim, self.action_dim)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict()) # initial target net weights from policy net\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict()) # initial target net weights from value net\n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.lr)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.lr)\n",
    "    \n",
    "    self.max_action = self.env.action_space.high[0]\n",
    "    self.min_action = self.env.action_space.low[0]\n",
    "    self.axis = (self.min_action + self.max_action) / 2\n",
    "    self.scale = (self.min_action - self.max_action) / 2\n",
    "    self.noise = OUNoise(self.action_dim, theta=0.15, sigma=0.1) # import noise # smaller noise\n",
    "    # hex smaller hexagons and change res\n",
    "\n",
    "    self.last_score_plot = [0]\n",
    "    self.avg_score_plot = [0]\n",
    "\n",
    "    self.memory_main = Memory(memory_size=5000)\n",
    "    self.memory_good_act = Memory(memory_size=5000)\n",
    "\n",
    "    self.loss_check = []\n",
    "    self.rider_num = []\n",
    "\n",
    "    pass\n",
    "\n",
    "  def get_action(self, actor_net, state):\n",
    "      if not isinstance(state, torch.Tensor):\n",
    "        state = torch.from_numpy(state).float()\n",
    "      action = actor_net(state)\n",
    "      return action\n",
    "    \n",
    "\n",
    "  def get_q_value(self, critic_net, state, action):\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "      state = torch.from_numpy(state).float()\n",
    "    if not isinstance(action, torch.Tensor):\n",
    "      action = torch.from_numpy(action).float()\n",
    "    q_value = critic_net(state, action)\n",
    "    return q_value\n",
    "\n",
    "  def update_actor(self, state):\n",
    "    action = self.actor(state)\n",
    "    #action = torch.clamp(action, float(self.min_action), float(self.max_action))\n",
    "    q_value = -torch.mean(self.critic(state, action)) \n",
    "    self.actor_optimizer.zero_grad() # calculate the gradient to update actor\n",
    "    q_value.backward()\n",
    "    self.actor_optimizer.step()\n",
    "    pass\n",
    "\n",
    "  def update_critic(self, state, action, target):\n",
    "    q_value = self.critic(state, action)\n",
    "    loss = F.mse_loss(q_value, target) # minimize loss to update critic\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "    check = loss.detach().numpy()\n",
    "    self.loss_check.append(check)\n",
    "    pass\n",
    "\n",
    "  def soft_update(self, target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "      target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau) # weights target and source\n",
    "\n",
    "  def draw_fig(self):\n",
    "    plt.plot(self.last_score_plot, '-')\n",
    "    plt.plot(self.avg_score_plot, 'r-')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reinforcement Learning Process')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "  def run_ddpg(self):\n",
    "    state = self.env.reset()\n",
    "\n",
    "    iteration_now = 0\n",
    "    iteration = 0\n",
    "    episode = 0\n",
    "    episode_score = 0\n",
    "    episode_steps = 0\n",
    "\n",
    "    memory_warmup = self.args.batch_size * 3\n",
    "\n",
    "    self.noise.reset()\n",
    "\n",
    "    while episode < self.args.max_episode:\n",
    "      print('\\rIteration {} | Episode {} | Result -> '.format(iteration_now, episode), end='')\n",
    "      #action = np.array([1, 1, 1, 1, 1, 1, 1])\n",
    "      action = self.get_action(self.actor, state).detach().numpy()\n",
    "\n",
    "      # blend determinstic action with random action during exploration, noise will become samller during the process\n",
    "      if episode < self.args.max_explore_eps:\n",
    "        p = episode / self.args.max_explore_eps\n",
    "        action = action * p + (1 - p) * self.noise.sample()\n",
    "        #action = action* 0.5 + 0.5 * self.noise.sample()\n",
    "      self.rider_num.append(state[0])\n",
    "      action = np.clip(action, -1, 1) # select valid action range\n",
    "      next_state, reward, done = self.env.step(action)\n",
    "      reward = reward # re-sacle the reward for plot\n",
    "      self.memory_main.append([state, action, reward, next_state, done])\n",
    "\n",
    "      if iteration >= memory_warmup:\n",
    "        memory_batch = self.memory_main.sample_batch(int(self.args.batch_size))\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = map(lambda x: torch.tensor(x).float(), zip(*memory_batch))\n",
    "        action_next = self.get_action(self.actor_target, next_state_batch)\n",
    "\n",
    "        # using discounted reward as target q-value to update critic\n",
    "        Q_next = self.get_q_value(self.critic_target, next_state_batch, action_next).detach()\n",
    "        Q_target_batch = reward_batch[:, None] + self.args.gamma * (1 - done_batch[:, None]) * Q_next\n",
    "        self.update_critic(state_batch, action_batch, Q_target_batch)\n",
    "        self.update_actor(state_batch)\n",
    "\n",
    "        # soft update\n",
    "        self.soft_update(self.actor_target, self.actor, self.args.tau)\n",
    "        self.soft_update(self.critic_target, self.critic, self.args.tau)\n",
    "\n",
    "      episode_score += reward\n",
    "      episode_steps += 1\n",
    "      iteration_now += 1\n",
    "      iteration += 1\n",
    "\n",
    "      if done:\n",
    "        print('Episode {:03d} | Episode Score:{:.03f} | steps {}'.format(episode, episode_score, episode_steps))\n",
    "        #print(f'Policy now: {radius}')\n",
    "        self.avg_score_plot.append(self.avg_score_plot[-1] * 0.99 + episode_score * 0.01)\n",
    "        self.last_score_plot.append(episode_score)\n",
    "\n",
    "        episode += 1\n",
    "        episode_score = 0\n",
    "        episode_steps = 0\n",
    "        iteration_now = 0\n",
    "\n",
    "        state = self.env.reset()\n",
    "        self.noise.reset()\n",
    "\n",
    "        #plt.plot(self.rider_num, '-')\n",
    "        #plt.show()\n",
    "        self.rider_num=[]\n",
    "      else:\n",
    "        state = next_state # state tranist\n",
    "\n",
    "    #drawnow(self.draw_fig) # drawnow function is for dynamic update\n",
    "    self.draw_fig()\n",
    "    return state, self.actor\n",
    "  \n",
    "  def debug_info(self):\n",
    "    return self.loss_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhh\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3850 | Episode 0 | Result -> Episode 000 | Episode Score:98.827 | steps 3851\n",
      "Iteration 505 | Episode 1 | Result -> Episode 001 | Episode Score:99.839 | steps 506\n",
      "Iteration 144 | Episode 2 | Result -> Episode 002 | Episode Score:99.952 | steps 145\n",
      "Iteration 10823 | Episode 3 | Result -> Episode 003 | Episode Score:96.597 | steps 10824\n",
      "Iteration 1046 | Episode 4 | Result -> Episode 004 | Episode Score:99.699 | steps 1047\n",
      "Iteration 6140 | Episode 5 | Result -> Episode 005 | Episode Score:98.016 | steps 6141\n",
      "Iteration 4624 | Episode 6 | Result -> Episode 006 | Episode Score:98.534 | steps 4625\n",
      "Iteration 12 | Episode 7 | Result -> Episode 007 | Episode Score:99.992 | steps 13\n",
      "Iteration 7437 | Episode 8 | Result -> Episode 008 | Episode Score:96.844 | steps 7438\n",
      "Iteration 1170 | Episode 9 | Result -> Episode 009 | Episode Score:99.372 | steps 1171\n",
      "Iteration 8686 | Episode 10 | Result -> Episode 010 | Episode Score:96.442 | steps 8687\n",
      "Iteration 61 | Episode 11 | Result -> Episode 011 | Episode Score:99.965 | steps 62\n",
      "Iteration 2740 | Episode 12 | Result -> Episode 012 | Episode Score:98.503 | steps 2741\n",
      "Iteration 3008 | Episode 13 | Result -> Episode 013 | Episode Score:97.828 | steps 3009\n",
      "Iteration 17 | Episode 14 | Result -> Episode 014 | Episode Score:99.993 | steps 18\n",
      "Iteration 29271 | Episode 15 | Result -> Episode 015 | Episode Score:77.524 | steps 29272\n",
      "Iteration 317 | Episode 16 | Result -> Episode 016 | Episode Score:99.645 | steps 318\n",
      "Iteration 9438 | Episode 17 | Result -> Episode 017 | Episode Score:92.104 | steps 9439\n",
      "Iteration 54980 | Episode 18 | Result -> Episode 018 | Episode Score:47.090 | steps 54981\n",
      "Iteration 5222 | Episode 19 | Result -> Episode 019 | Episode Score:94.370 | steps 5223\n",
      "Iteration 24485 | Episode 20 | Result -> Episode 020 | Episode Score:73.468 | steps 24486\n",
      "Iteration 44480 | Episode 21 | Result -> Episode 021 | Episode Score:46.735 | steps 44481\n",
      "Iteration 75398 | Episode 22 | Result -> "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args(args\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m     11\u001b[0m ddpg \u001b[38;5;241m=\u001b[39m DDPG(args)\n\u001b[1;32m---> 12\u001b[0m last_state, policy \u001b[38;5;241m=\u001b[39m \u001b[43mddpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_ddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 188\u001b[0m, in \u001b[0;36mDDPG.run_ddpg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m memory_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_main\u001b[38;5;241m.\u001b[39msample_batch(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size))\n\u001b[0;32m    187\u001b[0m state_batch, action_batch, reward_batch, next_state_batch, done_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mtensor(x)\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mmemory_batch))\n\u001b[1;32m--> 188\u001b[0m action_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# using discounted reward as target q-value to update critic\u001b[39;00m\n\u001b[0;32m    191\u001b[0m Q_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_q_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target, next_state_batch, action_next)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[1;32mIn[56], line 112\u001b[0m, in \u001b[0;36mDDPG.get_action\u001b[1;34m(self, actor_net, state)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    111\u001b[0m   state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m--> 112\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mactor_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[56], line 48\u001b[0m, in \u001b[0;36mActor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 48\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m   out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_2(out))\n\u001b[0;32m     50\u001b[0m   out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(out))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:1591\u001b[0m, in \u001b[0;36melu\u001b[1;34m(input, alpha, inplace)\u001b[0m\n\u001b[0;32m   1589\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39melu_(\u001b[38;5;28minput\u001b[39m, alpha)\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch DDPG solution of Ride_hailing Radius')\n",
    "parser.add_argument('--gamma', type=float, default=0.99)\n",
    "parser.add_argument('--lr', type=float, default=0.0001) # default 0.0001\n",
    "parser.add_argument('--tau', type=float, default=0.001) # critic output weights between critic and target networks, default 0.001\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--max_episode', type=int, default=200)\n",
    "parser.add_argument('--max_explore_eps', type=int, default=150)\n",
    "parser.add_argument('--hr_time', type=int, default=18)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "ddpg = DDPG(args)\n",
    "last_state, policy = ddpg.run_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = policy(torch.Tensor(np.array([0.9, 0.4, 0.1, 0.23])))\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9464041e-03, 5.0441520e-03, 5.1186103e-03, ..., 5.4286823e+00,\n",
       "       1.3515896e+01, 5.3616745e+01], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ddpg.debug_info()\n",
    "loss = np.array(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
